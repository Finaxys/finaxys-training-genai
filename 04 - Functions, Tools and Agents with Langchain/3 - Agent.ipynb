{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc40496-3127-48b9-b1f4-49c192795a1c",
   "metadata": {},
   "source": [
    "# Agents\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce474cb5-39ae-4d40-b827-a3aef3b52d04",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d95f63-d36b-4d45-adcc-90ee27b8aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98747d67-294a-4983-a9a8-46b250c643ae",
   "metadata": {},
   "source": [
    "## Define the agent\n",
    "We first need to create our agent. This is the chain responsible for determining what action to take next.\n",
    "\n",
    "In this example, we will use OpenAI Function Calling to create this agent. This is generally the most reliable way to create agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa99b33-eed6-48af-96eb-682bb18d7a48",
   "metadata": {},
   "source": [
    "First, let’s load the language model we’re going to use to control the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfc6ae-974b-4084-96bc-16940f6a82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2f9b8-fc2c-4103-a99b-f44e381904e0",
   "metadata": {},
   "source": [
    "We can see that it struggles to count the letters in the string “educa”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae44b46-308f-4b1c-abb5-ba644bb1214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"how many letters in the word 'educa'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2b901-49e2-437c-8f06-d54db5fd3964",
   "metadata": {},
   "source": [
    "Next, let’s define some tools to use. Let’s write a really simple Python function to calculate the length of a word that is passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feade1ab-c213-49be-9112-40580c5329d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9fc54-2376-4df9-a2e7-edd741416e8b",
   "metadata": {},
   "source": [
    "Now let us create the prompt. Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: input and agent_scratchpad. input should be a string containing the user objective. agent_scratchpad should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95048cdc-e95b-4bec-aaa0-0ec8c87620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b5a6d-fb03-4499-af41-c8a03318e244",
   "metadata": {},
   "source": [
    "How does the agent know what tools it can use? In this case we’re relying on OpenAI function calling LLMs, which take functions as a separate argument and have been specifically trained to know when to invoke those functions.\n",
    "\n",
    "To pass in our tools to the agent, we just need to format them to the OpenAI function format and pass them to our model. (By bind-ing the functions, we’re making sure that they’re passed in each time the model is invoked.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903ffff-5307-4cf2-b949-c5f12088b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77a5a0-c9de-4c8b-83c5-a0193995d4ef",
   "metadata": {},
   "source": [
    "Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8d589-52ce-4e5e-bc7a-b65a5f429204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0f106-9a50-4de9-acc5-7fa4ec3cb305",
   "metadata": {},
   "source": [
    "Now that we have our agent, let’s play around with it! Let’s pass in a simple question and empty intermediate steps and see what it returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560af42-148f-425b-a9c6-a7a755ca97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"input\": \"how many letters in the word educa?\", \"intermediate_steps\": []})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1709174-9a05-4b3d-98e0-1c1961dce980",
   "metadata": {},
   "source": [
    "## Define the runtime\n",
    "So this is just the first step - now we need to write a runtime for this. The simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an AgentFinish is returned. Let’s code that up below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e62fe-6c45-4336-8a48-cfde6dbee031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "user_input = \"how many letters in the word educa?\"\n",
    "intermediate_steps = []\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": user_input,\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        print(f\"TOOL NAME: {output.tool}\")\n",
    "        print(f\"TOOL INPUT: {output.tool_input}\")\n",
    "        tool = {\"get_word_length\": get_word_length}[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        intermediate_steps.append((output, observation))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca4df7-0b8b-44c6-b4d7-a7eb85c7817a",
   "metadata": {},
   "source": [
    "Woo! It’s working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecf751-eed1-42cc-9b2a-3c949f6b7a02",
   "metadata": {},
   "source": [
    "### Using AgentExecutor\n",
    "To simplify this a bit, we can import and use the AgentExecutor class. This bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd42a6e-003f-4a36-8db5-84b563b78f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795d0e3-eed3-435e-b97c-bc8b4f911a32",
   "metadata": {},
   "source": [
    "Now let’s test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048213d2-25e2-46af-9ab6-b830794c5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how many letters in the word educa?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba90e0-2cc0-4666-8eec-dc302943ed81",
   "metadata": {},
   "source": [
    "## Adding memory\n",
    "This is great - we have an agent! However, this agent is stateless - it doesn’t remember anything about previous interactions. This means you can’t ask follow up questions easily. Let’s fix that by adding in memory.\n",
    "\n",
    "In order to do this, we need to do two things:\n",
    "\n",
    "Add a place for memory variables to go in the prompt\n",
    "Keep track of the chat history\n",
    "First, let’s add a place for memory in the prompt. We do this by adding a placeholder for messages with the key \"chat_history\". Notice that we put this ABOVE the new user input (to follow the conversation flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90b3ee-c257-4903-bea6-ee9fe5020cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5b71c-f004-4f79-8f18-17c4b1fa319f",
   "metadata": {},
   "source": [
    "We can then set up a list to track the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465a6fa-eca6-4569-ab69-149d692b42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde6c52-69d0-4719-9587-a3a7815f736d",
   "metadata": {},
   "source": [
    "We can then put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d3440-6711-4e27-bda4-87c7ab9327ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845e1ef-37ee-4474-9d28-1b741a52da0e",
   "metadata": {},
   "source": [
    "When running, we now need to track the inputs and outputs as chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131a803-b01f-415e-8506-3954de53645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"how many letters in the word educa?\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31e556-1e09-4d9c-83a0-d314d45be427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.docstore import Wikipedia\n",
    "from langchain import hub\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "# retriever = WikipediaRetriever()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"useful for when you need to ask with search\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"useful for when you need to ask with lookup\",\n",
    "    ),\n",
    "]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "llm = OpenAI()\n",
    "agent = create_react_agent(llm, tools, prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f8a4d-82bb-4f3d-9d58-218ebf201db9",
   "metadata": {},
   "source": [
    "# Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee711b9-3a8b-4f1e-8c53-7604f0633c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8419c7-f4ec-40a8-9dbc-9a03765c0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?\"\n",
    "agent_executor.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba602b-cb83-4a7c-bff2-d9448e9eaa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
