{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of62xpmjWPGa"
   },
   "source": [
    "# L7 Construire un système de bout en bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "#### Chargement de la clé OpenAI et des librairies Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils\n",
    "\n",
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-4o-mini\", temperature=0, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Système de prompts en chaîne pour traiter la demande de l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Étape 1 : L'entrée a passé la vérification de modération.\n",
      "Error: Invalid JSON string\n",
      "Étape 2: Extraire la liste des produits\n",
      "Étape 3 : Informations sur le produit recherchées.\n",
      "Étape 4 : Réponse générée à la question de l'utilisateur.\n",
      "Étape 5 : Réponse ayant passé la vérification de modération.\n",
      "Étape 6 : Le modèle a évalué la réponse.\n",
      "Étape 7 : Le modèle a désapprouvé la réponse.\n",
      "Je ne peux pas fournir les informations que vous recherchez. Je vais vous mettre en relation avec un représentant humain pour une assistance supplémentaire. \n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Étape 1 : Vérifiez l'entrée pour voir si elle signale l'API de modération ou s'il s'agit d'une injection de prompt.\n",
    "    response = client.moderations.create(input=user_input)\n",
    "    moderation_output = response.results[0]\n",
    "\n",
    "    if moderation_output.flagged:\n",
    "        print(\"Étape 1 : Entrée signalée par l'API de modération. \")\n",
    "        return \"Désolé, nous ne pouvons traiter votre requête.\"\n",
    "\n",
    "    if debug: print(\"Étape 1 : L'entrée a passé la vérification de modération.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    category_and_product_response = '\\n'.join([line for line in category_and_product_response.split('\\n') if not line.strip().startswith('```')])\n",
    "    # Étape 2: Extraire la liste des produits\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Étape 2 : Extraire la liste des produits\")\n",
    "\n",
    "    # Étape 3 : Si des produits sont trouvés on les récupère\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Étape 3 : Informations sur le produit recherchées.\")\n",
    "\n",
    "    # Étape 4: Répondre à la question de l'utilisateur\n",
    "    system_message = f\"\"\"Vous êtes un assistant du service client pour un \n",
    "    grand magasin d'électronique. Répondez avec un ton amical et serviable, \n",
    "    en fournissant des réponses concises. Assurez-vous de poser à l'utilisateur \n",
    "    des questions de suivi pertinentes.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Informations intéressantes pour le produit :\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Étape 4 : Réponse générée à la question de l'utilisateur.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Étape 5: Soumettez la réponse à l'API de modération.\n",
    "    response = client.moderations.create(input=final_response)\n",
    "    moderation_output = response.results[0]\n",
    "\n",
    "    if moderation_output.flagged:\n",
    "        if debug: print(\"Étape 5 : Réponse signalée par l'API de modération.\")\n",
    "        return \"Désolé, nous ne pouvons pas fournir cette information.\"\n",
    "\n",
    "    if debug: print(\"Étape 5 : Réponse ayant passé la vérification de modération.\")\n",
    "\n",
    "    # Étape 6 : Demandez au modèle si la réponse répond bien à la demande initiale de l'utilisateur.\n",
    "    user_message = f\"\"\"\n",
    "    Message de l'utilisateur : {delimiter}{user_input}{delimiter}\n",
    "    Réponse de l'agent : {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    La réponse répond-elle suffisamment à la question ?\n",
    "\n",
    "    Répondez par \"Y\" ou \"N\".\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Étape 6 : Le modèle a évalué la réponse.\")\n",
    "\n",
    "    # Étape 7 : Si oui, utilisez cette réponse ; sinon, dites que vous allez mettre l'utilisateur en relation avec un humain. \n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Étape 7 : Le modèle a approuvé la réponse.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Étape 7 : Le modèle a désapprouvé la réponse.\")\n",
    "        neg_str = \"Je ne peux pas fournir les informations que vous recherchez. Je vais vous mettre en relation avec un représentant humain pour une assistance supplémentaire. \"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"Parlez-moi du SmartX ProPhone et de l'appareil photo FotoSnap, le reflex numérique. Parlez-moi également de vos téléviseurs.\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction qui collecte les messages de l'utilisateur et de l'assistant au fil du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_messages(debug=False):\n",
    "    user_input = inp.value_input\n",
    "    if debug: print(f\"User Input = {user_input}\")\n",
    "    if user_input == \"\":\n",
    "        return\n",
    "    inp.value = ''\n",
    "    global context\n",
    "    #response, context = process_user_message(user_input, context, utils.get_products_and_category(),debug=True)\n",
    "    response, context = process_user_message(user_input, context, debug=False)\n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(user_input, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, styles={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discutez avec le chatbot !\n",
    "Notez que le message système inclut des instructions détaillées sur ce que l'OrderBot doit faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"Vous êtes un assistant de service.\"} ]  \n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Entrez votre texte ici …')\n",
    "button_conversation = pn.widgets.Button(name=\"Assistant\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
